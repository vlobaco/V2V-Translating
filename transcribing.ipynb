{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/victor/git/Transcribing/.venv/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/Users/victor/git/Transcribing/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[aist#0:0/pcm_s16le @ 0x141e2c900] Guessed Channel Layout: mono\n",
      "Input #0, wav, from '/private/var/folders/yv/vkckswq97tv3p5t63sn369v40000gn/T/gradio/b33c090e94dd61a1f60add5eb81bcb5e8241acd4/audio.wav':\n",
      "  Duration: 00:00:06.36, bitrate: 705 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'tmp.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.1.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.3.100 pcm_s16le\n",
      "[out#0/wav @ 0x141e2f650] video:0KiB audio:199KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.038325%\n",
      "size=     199KiB time=00:00:06.36 bitrate= 256.1kbits/s speed=1.13e+03x    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaab475e9ba41aab20eef47795d436a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fr_speaker_5_semantic_prompt.npy:   0%|          | 0.00/4.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830aeac52d5407ca1f2e9f808e534b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fr_speaker_5_coarse_prompt.npy:   0%|          | 0.00/13.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e0fc7d55cb4f8a99fb4d2f8f79a0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fr_speaker_5_fine_prompt.npy:   0%|          | 0.00/26.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "/Users/victor/git/Transcribing/.venv/lib/python3.9/site-packages/gradio/processing_utils.py:583: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "ffmpeg version 7.0.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[aist#0:0/pcm_s16le @ 0x14c62d9e0] Guessed Channel Layout: mono\n",
      "Input #0, wav, from '/private/var/folders/yv/vkckswq97tv3p5t63sn369v40000gn/T/gradio/e9a285ec418ef1b4ae2fa259cf2054972d061aff/audio.wav':\n",
      "  Duration: 00:00:14.34, bitrate: 705 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'tmp.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.1.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.3.100 pcm_s16le\n",
      "[out#0/wav @ 0x14c62db50] video:0KiB audio:448KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.016998%\n",
      "size=     448KiB time=00:00:14.34 bitrate= 256.0kbits/s speed=1.88e+03x    \n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "ffmpeg version 7.0.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[aist#0:0/pcm_s16le @ 0x15af26920] Guessed Channel Layout: mono\n",
      "Input #0, wav, from '/private/var/folders/yv/vkckswq97tv3p5t63sn369v40000gn/T/gradio/e1a370980bce0684e22402de9e72f65424b9ae27/audio.wav':\n",
      "  Duration: 00:00:14.40, bitrate: 705 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'tmp.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.1.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.3.100 pcm_s16le\n",
      "[out#0/wav @ 0x15b8096d0] video:0KiB audio:450KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.016927%\n",
      "size=     450KiB time=00:00:14.40 bitrate= 256.0kbits/s speed=1.75e+03x    \n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import wave, struct\n",
    "from transformers import pipeline, AutoProcessor, BarkModel\n",
    "\n",
    "# Setup channel info\n",
    "RATE = 16000 # Sample Rate\n",
    "CHUNK = 1024 # Block Size\n",
    "\n",
    "# load model and processor for v2t\n",
    "pipe_v2t = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# load model and processor for t2v\n",
    "model_id = \"suno/bark-small\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model = BarkModel.from_pretrained(model_id)\n",
    "model = model.to_bettertransformer()\n",
    "#model.enable_cpu_offload()\n",
    "sample_rate = model.generation_config.sample_rate\n",
    "\n",
    "# load model for translation\n",
    "pipe_translation = {\n",
    "    'en': {\n",
    "        'translator': pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\"),\n",
    "        'voice_preset': \"v2/en_speaker_5\"\n",
    "    },\n",
    "    'fr': {\n",
    "        'translator': pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-fr\"),\n",
    "        'voice_preset': \"v2/fr_speaker_5\"\n",
    "    },\n",
    "    'ru': {\n",
    "        'translator': pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-ru\"),\n",
    "        'voice_preset': \"v2/ru_speaker_5\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def transcribe(filepath, target_lang='en'):\n",
    "    WAVE_OUTPUT_FILENAME = 'tmp.wav'\n",
    "    output_str = f'/opt/homebrew/bin/ffmpeg -y -i {filepath} -ar {RATE} {WAVE_OUTPUT_FILENAME}'\n",
    "    subprocess.run(output_str, shell=True)\n",
    "\n",
    "    # open the file for reading.\n",
    "    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'rb')\n",
    "\n",
    "    n_frames = waveFile.getnframes()\n",
    "    frames = []\n",
    "    # looping from beginning of file to the end\n",
    "    for _ in range(n_frames):\n",
    "        data = waveFile.readframes(1)\n",
    "        frames.extend(struct.unpack(f'<h', data))\n",
    "\n",
    "    # cleanup stuff.\n",
    "    waveFile.close()\n",
    "    os.remove(WAVE_OUTPUT_FILENAME)\n",
    "    array = np.array(frames, dtype=np.float16)\n",
    "    array = array/32768.0\n",
    "\n",
    "    output = pipe_v2t(array,\n",
    "        max_new_tokens = 256,\n",
    "        generate_kwargs = {\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"spanish\",\n",
    "        }\n",
    "    )\n",
    "    output = output['text']\n",
    "\n",
    "    translation_output = pipe_translation[target_lang]['translator'](output, max_length=256)[0]['translation_text']\n",
    "\n",
    "    inputs = processor(translation_output, voice_preset=pipe_translation[target_lang]['voice_preset'])\n",
    "    audio_array = model.generate(**inputs)\n",
    "    audio_array = audio_array.cpu().numpy().squeeze()\n",
    "    return translation_output, (sample_rate, audio_array)\n",
    "\n",
    "\n",
    "choices = [('English', 'en'), ('French', 'fr'), ('Russian', 'ru')]\n",
    "interface = gr.Interface(fn=transcribe, \n",
    "                         inputs=[\n",
    "                             gr.Audio(sources='microphone', type='filepath'),\n",
    "                             gr.Dropdown(choices=choices, label='Translate to'),\n",
    "                             ],\n",
    "                         outputs=[\n",
    "                             gr.Textbox(label='Voice to Text'), \n",
    "                             gr.Audio(label='Read Aloud', autoplay=True, visible=False)\n",
    "                             ],\n",
    "                         allow_flagging=\"never\",\n",
    "                         )\n",
    "\n",
    "gr.close_all()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio # Soundcard audio I/O access library\n",
    "import wave # Python 3 module for reading / writing simple .wav files\n",
    "\n",
    "# Setup channel info\n",
    "FORMAT = pyaudio.paInt16 # data type format\n",
    "CHANNELS = 1 # Adjust to your number of channels\n",
    "RATE = 16000 # Sample Rate\n",
    "CHUNK = 1024 # Block Size\n",
    "RECORD_SECONDS = 15 # Record time\n",
    "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording...\n",
      "finished recording\n"
     ]
    }
   ],
   "source": [
    "# Startup pyaudio instance\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                rate=RATE, input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "print (\"recording...\")\n",
    "frames = []\n",
    "\n",
    "# Record for RECORD_SECONDS\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "print (\"finished recording\")\n",
    "\n",
    "\n",
    "# Stop Recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Write your new .wav file with built in Python 3 Wave module\n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(frames))\n",
    "waveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "# open the file for reading.\n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'rb')\n",
    "\n",
    "# create an audio object\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# open stream based on the wave object which has been input.\n",
    "stream = audio.open(format = FORMAT,\n",
    "                channels = CHANNELS,\n",
    "                rate = RATE,\n",
    "                output = True)\n",
    "\n",
    "# read data (based on the chunk size)\n",
    "data = waveFile.readframes(CHUNK)\n",
    "\n",
    "# play stream (looping from beginning of file to the end)\n",
    "while data:\n",
    "    # writing to the stream is what *actually* plays the sound.\n",
    "    stream.write(data)\n",
    "    data = waveFile.readframes(CHUNK)\n",
    "\n",
    "# cleanup stuff.\n",
    "waveFile.close()\n",
    "stream.close()    \n",
    "audio.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.001983642578125, -0.002777099609375, -0.00250244140625, -0.004486083984375, -0.00518798828125, -0.005859375, -0.0074462890625, -0.007568359375, -0.00732421875, -0.00885009765625]\n"
     ]
    }
   ],
   "source": [
    "import wave, struct\n",
    "\n",
    "# open the file for reading.\n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'rb')\n",
    "\n",
    "# read data (based on the chunk size)\n",
    "data = waveFile.readframes(CHUNK)\n",
    "\n",
    "frames = []\n",
    "# play stream (looping from beginning of file to the end)\n",
    "while data:\n",
    "    # writing to the stream is what *actually* plays the sound.\n",
    "    frames.extend(struct.unpack(f'<{CHUNK}h', data))\n",
    "    data = waveFile.readframes(CHUNK)\n",
    "\n",
    "# cleanup stuff.\n",
    "waveFile.close()\n",
    "\n",
    "frames = [frame/32768.0 for frame in frames]\n",
    "\n",
    "print(frames[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se han detectado un elemento ISR y un puesto de mando enemigo de entidad grupo táctico. Identifica tres líneas de acción posibles.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
    "\n",
    "# load streaming dataset and read first audio sample\n",
    "input_features = processor(frames, sampling_rate=RATE, return_tensors=\"pt\").input_features\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "print(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the transcription to a file\n",
    "with open(\"transcription.txt\", \"w\") as file:\n",
    "    file.write(transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the transcription from the file\n",
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "    transcription = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BarkModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "model = BarkModel.from_pretrained(\"suno/bark\")\n",
    "\n",
    "voice_preset = \"v2/es_speaker_5\"\n",
    "\n",
    "inputs = processor(transcription, voice_preset=voice_preset, )\n",
    "\n",
    "audio_array = model.generate(**inputs)\n",
    "audio_array = audio_array.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(['¿Cuánto tiempo tomará esto?'], voice_preset=voice_preset, )\n",
    "\n",
    "audio_array = model.generate(**inputs)\n",
    "audio_array = audio_array.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio, struct\n",
    "\n",
    "# create an audio object\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# open stream based on the wave object which has been input.\n",
    "stream = audio.open(format = FORMAT,\n",
    "                channels = CHANNELS,\n",
    "                rate = 24000,\n",
    "                output = True)\n",
    "\n",
    "# denormalize audio\n",
    "max_abs_array = max(abs(audio_array))\n",
    "normalized_array = audio_array / (max_abs_array * 2) * 32768\n",
    "normalized_array = normalized_array.astype('int16')\n",
    "\n",
    "# play stream (looping from beginning of file to the end)\n",
    "for index in range(0, len(normalized_array), CHUNK):\n",
    "    # writing to the stream is what *actually* plays the sound.\n",
    "    data = normalized_array[index:index+CHUNK]\n",
    "    data = struct.pack(f'<{len(data)}h', *data)\n",
    "    stream.write(data)\n",
    "\n",
    "# cleanup stuff.\n",
    "stream.close()    \n",
    "audio.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
